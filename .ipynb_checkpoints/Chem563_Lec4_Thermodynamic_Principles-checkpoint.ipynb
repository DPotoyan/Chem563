{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Lecture 4\n",
    "* #### Review of the main principles of thermodynamics. \n",
    "*  \\#Equilibrium vs Non-equilibrium \\# Thermodynamic variables \\#Extensive vs Intensive  \\# Fundamental equation vs state equation \\#Quasistatic process and reversibility \\#Adiabatic vs diathermal trnasformation \\#Constraints  \\#Legendre transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "<br>\n",
    "<div class=\"alert-info\">\n",
    "<ul>\n",
    "<li> Thermodynamics is a phenomoneological theory: it describes macrosocpic phenomena in terms of quanitites that can be observed and measrued by macroscopic devices.  \n",
    "<li> Task of thermodynamics is predicting transformations of equilibrium state A to another equilibrium state B.\n",
    "<li> In equilibrium, macroscopic matter assumes a particularly simple description in terms of few extensive quantites.  \n",
    "<li> Extensive variables are a priviledges set of variables becasue they quantify degree of macroscopic matter. How much volume, energy, particles are contained in pieace of matter \n",
    "<li>  Intensive variables are derived from extensive ones and are conjugate pairs to extensive ones. Meaning one can replace extensive variables by the functions of intensive variables. E.g via legendre transformation.\n",
    "<li>  Fundmanetal equation is the equation that binds together all extensive variables, e.g $E(U,V,S)$ or $E(U,V,S,N_1,N_2)$     \n",
    "<li>  Random variable, $X(\\omega)$ is a variable which assumes values corresponding to the elementary events $\\omega$ <br> [Example: X = an x amount of money won/lost when die is even/odd] <div/>\n",
    "<li>  Stochastic process is a time evolution of a random variable $X(\\omega_t)$ <br > [Example: consecutive throwing of a die, value of stock market over years]  \n",
    "<li>  Probability distribution $P_X(x)=$\"probability that X=x\": quantifies probablity of values x being realized by X. <br>\n",
    "<li>  Probability distribution can be discrete (binomial) or continuous (Gaussian, Poisson) \n",
    "    </ul> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability is part of our everyday language weather we are talking about science or not. At one time or another we may have asked: What is the probability of raining tomorrow? What is probability of measring spin state of electron to +1/2? what is probability of pulling out a queen of spades from the deck? Probabilites are quantiative expressions of our confidents in the events. The mathematical theory of probability provides the toolbox for quantifying probabilities of various possible events. \n",
    "<br>The mathematical foundation of probability theory as we know it today were established by  by A.N. Komogorov who showed that probabilites can be defined in a logically menainfgul and consisted way by the sets of events. The smallest points in the sets of events are the atoms of probability they are known as the elementary events. These are essentially events which can not be analyzed further for whatever reason be it experimental resolution, our deliberate ignorance or other fundmanetal reason. For instance when rolling a die, different numbers (1,2,3,4,5,6) will naturally be our elementary events. When drawing a deck different cards will be our elementary events. However we can also have events grouped by \"odd\" numbers for die or \"card color\" or \"ace\" etc. Those would be non-elementary or composite events. Having defined sets of events we can proceed to quantify the confidence we have in them by assinging some measure of probability to each events. The probability so defined must satisfy a number of rules which we state briefly below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Probability Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization\n",
    "Probability over set of all possible events must be one:\n",
    "\n",
    "$$P(\\Omega)=1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule of Addition\n",
    "For any two events that do not overlap we have probability of <em>either</em> A or B happening as the sum of probabilities for individual events A and B:\n",
    "\n",
    "$$P(A \\cup B)=P(A)+P(B)\\,\\, if\\,\\, A \\cap B=\\emptyset$$\n",
    "\n",
    "In case our sample space $\\Omega$ is broken up into n non-overlapping sets then:\n",
    "\n",
    "$$ P(\\Omega)=P(A_1 \\cup A_2\\cup A_3 ... \\cup A_n)=\\sum_n P(A_n)=1$$\n",
    "\n",
    "For any event $A$ we can define another event as \"all the other event except A\" denoted by $\\bar A$. These events have no overlap $A \\cap \\bar A=\\emptyset$ and $A \\cup \\bar A=\\Omega$. Hence using rule of addition for disjoint events:\n",
    "$$P(\\bar A)=1-P(A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes theorem!\n",
    "Probability of an event when both A and B happen is given by:\n",
    "\n",
    "$$P(A,B)=P(A|B)P(B)=P(B|A)P(A)$$\n",
    "\n",
    "P(A|B) is a conditional probability: probability of event A conditioned on the information that event B has already happened. Bayes theorem has wide range of applications in sciences and is a major tool for refining hypothesis with the availible data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Random variable \n",
    "\n",
    "Random variable X is a mapping from a set of events in to real numbers $X:\\,\\Omega \\rightarrow R$ . For example the events of flipped coin showing H or T we can assign to numeric values (e.g +1, -1) which would then be the realizations of a random variable X. Some other examples of a random variables to think of are: velocities of molecules in air, a dihedral angle in a protein moelcule, number of birds on a tree in a given day, stock market, etc. Random variables are characterized by probabilities of their realizations. In the example of random walk we have designated the net displacement of our walker, X as the random variable of primary interest. Then we proceeded to obtain prbability distirbution function P(x) which showed which values of X were likely. E.g we found that for a $p=q=1/2$ random walk X=0 value has the maximuam probability.\n",
    "\n",
    "#### Stochastic process\n",
    "Observing random variable over time defins a stochastic process. Random walk over time, which is the same as diffusion is a classic example of a stochastic process. Mathematically stochastic process is defined as a random variable which in addition to being defined over sample space $\\Omega$ also depends on time, $X(\\omega, t)$ denoted as $X(t)$ for simplicity. This notation just means that at differnet times the random variable assumes different values. Again think of bronwian particle and stocmarket for a quick example of a stochastic process. \n",
    "\n",
    "#### Cumulative distirbution function(cdf) and probability mass function (pmf)\n",
    "\n",
    "Probability of a random variable $X$ having values less than $X<x$ is gien by a cumulative distirbution function(cdf):\n",
    "\n",
    "$$Q(x)=\\{X(\\omega)<x \\}$$\n",
    "Cumulative distirbution function is an increasing function of x and should tend to 0 and 1 in limits $x=\\infty$ and $x=-\\infty$ respectively. \n",
    "\n",
    "Probability mass function, also often referred to as probability density or probability distribution is defined as the derivative of cumulant distirbution function and quantifes probability of X taking up values within $[x,x+dx]$ infinitesmal region of x.\n",
    "\n",
    "$$p(x)=\\frac{d Q(x)}{dx}$$\n",
    "We can also express cdf in terms of pmf:\n",
    "\n",
    "$$P(x)= \\int^{x}_{-\\infty} p(x) dx$$\n",
    "\n",
    "From wehich we get a recipie for computing probailities for arbitarry interval of values of X:\n",
    "\n",
    "$$P(a<x<b)= \\int^{b}_{a} p(x) dx $$\n",
    "\n",
    "In particular:\n",
    "\n",
    "$$P(-\\infty <x<+\\infty)= \\int^{-\\infty}_{+\\infty} p(x) dx=1 $$\n",
    "\n",
    "The last expression is a normalization conidtion that a proper probability distributions should satisfy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Expectations of probability distirbution function\n",
    "\n",
    "For a given probability distirbuton $p(x)$ of the expectation of function $f(x)$ is defined as:\n",
    "$$E[f]=\\langle f(x) \\rangle = \\int f(x)p(x)dx$$\n",
    "\n",
    "For more than one variable we have multivariate probability $P(x_1,x_2,...x_N)$ and expectation of a function of N vaiables $f(x_1,x_2,..x_N)$ can define in a similiar manner:\n",
    "$$E[f]=\\langle f(x_1,x_2,...x_N) \\rangle = \\int f(x_1,x_2,...x_N)P(x_1,x_2,...x_N)dx_1 dx_2... dx_N$$\n",
    "For discrete probability distributions integrals are replaced by sums:\n",
    "\n",
    "$$E[f]=\\sum_i f(n_i)P(n_i)$$\n",
    "\n",
    "$$E[f]=\\langle f(n_1,n_2,...n_N) \\rangle = \\sum_{n_1,n_2,...n_N} f(n_1,n_2,...n_N)p(n_1,n_2,...n_N)$$\n",
    "\n",
    "On a computer we inevitably have to deal with discrete probability distirbutions thus it is good to keep in mind the discrete version of the expressions that we write here. Needless to say continuous form is more convenient for analytical manipulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Means, variances, moments etc.\n",
    "Some forms of f(x) hold special significance and are known under special names. For instance expectation of f(x)=x is known as mean or average of x with respect to P(x):\n",
    "$$E[x]=\\langle x \\rangle = \\int xp(x)dx$$\n",
    "The expectation of square deviation $f(x)=(x-\\langle x\\rangle)^2$ is known as variance of x denoted by $\\sigma_x^2$. \n",
    "$$E[(x-\\langle x\\rangle)^2]= \\int \\big (x-\\langle x \\rangle \\big)^2p(x)dx=\\langle x^2\\rangle - \\langle x \\rangle^2$$\n",
    "The $\\sigma_x$ is called standard deviation. A low standard deviation indicates that the data points tend to be close to the mean, while a high standard deviation indicates that the data points are spread out over a wider range of values.  A useful property of the standard deviation is that, unlike the variance, it is expressed in the same units as the data. Expectations of pwers of x, $x^n$ are known as moments $\\mu_n$. Moments tell us some useful infromation about distirbutions. In particular the first moment $\\mu_1=\\mu=\\langle x \\rangle$ is the same as mean and higher moments contain valuable information about shape and symetry of the distribution. \n",
    "\n",
    "$$E[x^n]=\\mu_n=\\int x^n p(x)dx$$\n",
    "\n",
    "Note that often for comparing distirbution it is useful to consider centralized moments to get rid of the particular value of the mean and make distributions centered at 0. \n",
    "\n",
    "$$ E[(x-\\mu)^n]=\\int (x-\\mu)^n p(x)dx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating functions\n",
    "\n",
    "Another special expectation, known under the names of moment generating function or characteristic function is obtained by setting $f(x)=e^{ikx}$ or $f(x)=e^{kx}$. Physicst prefer the former since then moment generation function can also be viewed as a Fourier transform of probability distribution p(x). \n",
    "\n",
    "$$G_X(k)=\\langle e^{ikx} \\rangle = \\int p(x) e^{ikx} dx$$\n",
    "\n",
    "Expanding exponent in tayler series reveals why $G(k)$ is called a moment generating function:\n",
    "$$G_X(k)= \\int p(x) [1+ikx+(ikx)^2+...] dx=1+ik \\langle x \\rangle-k^2\\langle x^2 \\rangle $$\n",
    "And we get a new cool tool for computing moments:\n",
    "\n",
    "$$ (i)^n\\frac{dG_X(k)}{dk} \\Big |_{k=0} = \\langle x^n \\rangle $$\n",
    "\n",
    "Also for independent random variables X and Y we have a useful property that probability of X+Y transforms into simple product:\n",
    "\n",
    "$$G_{X+Y}(k)=\\langle e^{ik(x+y} \\rangle = \\langle e^{ikx} e^{ikx} \\rangle=\\langle e^{ikx} \\rangle \\langle e^{ikx}\\rangle= G_{X}(k) G_{Y}(k)$$\n",
    "\n",
    "For the case of random walk where our random variable was sum of n steps $X=\\sum^{n}_{i=1} X_i $ we find that: $G_{X}=G^n_{X_1}$ meaning that in order to characterize n step random walk we only need to compyte G for its one step. \n",
    "\n",
    "Another associated generating function is the cumulant generating function defined simply as:\n",
    "\n",
    "$$C_X(k)=lnG_X(k)=ln \\langle e^{ikx}\\rangle $$\n",
    "\n",
    "Utility of $C_X(k)$ is that it produces cumulants, $k_i$. Cumulats are analogus to moments in that they characterize distribution function and offer useful information about shape and symmetry of p(x). \n",
    "\n",
    "$$C_X(k)=lnG_X(k)=ln \\langle e^{ikx}\\rangle = \\sum_i \\kappa_i \\frac{(i k)^n}{n!}$$\n",
    "Another way of thinking of about cumulants is by noting that they are esentially moments of expansion in the exponent: $$\\langle e^{ikx}\\rangle=e^{\\sum_i \\kappa_i \\frac{(i k)^n}{n!} }$$\n",
    "\n",
    "The reason we care about cumulants is becasue they can have an advantage over using moments for a few reasons. Most notably for a gaussian function they yield a much more simpler result: $$\\kappa_1=\\mu,\\,\\,\\kappa_2=\\sigma^2\n",
    ",\\,  \\kappa_i=0\\,\\, for\\,\\, i\\geq3$$\n",
    "Thus gaussian distirbution is defined by only its first two cumulants. At last, cumulants for statistically independent random variables X and Y we have a simple relationship\n",
    "\n",
    "$$C_{X+Y}(k)=ln \\langle e^{ik(x+y)} \\rangle=ln \\langle e^{ikx} e^{iky} \\rangle=ln \\langle e^{ikx}\\rangle \\langle e^{iky} \\rangle= C_X(k)+C_Y(k) $$\n",
    "\n",
    "Now we can apply our newly learned tools to binomial distribution:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability by counting: the basics of combinatorics.\n",
    "We will see that in many problems of statistical mechanics and other sciences evauation of probability boiles down to counting number of possible arrangements. Problems of counting different arrangements is topic of combinaotrics. For our purposes we need to know just a few simple but incredibly useful expressions. <br>\n",
    "Ever wonder wow many ways we can shuffle letters in a word CAT? 3!=6 ways, you probably did not have to do any complex calculation to guess that. What about N letter words with distinct letters? Lets formulate this problem more precisely. How many ways distinct arrangements can one make out of sequence of N distingusihable objects? First object can be placed in any N palces, second in N(N-1) thrid in N(N-1)(N-2)...1 which is the definition of facotorial, hence we have a simple answer: N!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential selection of r objects from n Distingushable objects: $P^n_r$\n",
    "Imagine a different problem of placing r molcules to n empty lattice states or r distinct words on n sequence of n empty boxes. How many disting arrangements can we have? The first molecule can be assinged to any one of n sites, second n-1, third n-2 ... and the last can be placed among whats left n-r+1. Thus we have an answer that r disticnt objects can arranged among n sequence by $P^n_r$ number of ways:\n",
    "\n",
    "$$P^n_r = n(n-1)...(n-r+1)=\\frac{n!}{(n-r)!}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random selection of r objects from n Distingushable objects: $C^n_r$\n",
    "$P^n_r$ counts states where molecule 1 is in site-i and molecule 2 in site-j as distinct from molecule 2 in site-j and molecule 1 in site-i. What if molecules are indistiguishable? Well that there are r! less states. The answer is:\n",
    "$$C^n_r = \\frac{n!}{(n-r)!r!}$$\n",
    "For the random walk problem recall that all right or left steps were regarded as the same therefore we used $C^n_r$ for counting the number of sequences of steps with r total number of steps to the right. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial and binomial theorems\n",
    "We have seen examples of binomial therem in high school where we expanded nth power of $x_1+x_2$. This is a basic combinatorial task since we are combining different $x_1$ and $x_2$ terms when expanding product of n terms. Hence we can easily show that expansion coefficients are binomial coefficients intorduced above:\n",
    "$$(x_1+x_2)^n=\\sum^{n}_{r=0} \\frac{n!}{(n-r)! r!} x_1^{n-r}x_2^r$$ \n",
    "\n",
    "Once we are comfortable with binomial expansion we can generalize this result for multinomial expansion when we expand sum of arbitrary number of m terms:\n",
    "\n",
    "$$(x_1+x_2+...+x_m)^n=\\sum_{r_1+r_2+...r_m=n} \\frac{n!}{r_1! r_2! ... r_m!} x^{r_1}_1 x^{r_2}_2 ... x^{r_m}_m$$ \n",
    "\n",
    "This expansion is simply expressing the fact that when expanding any sum of terms we obtain all possible combination of terms with coefficients being number of ways terms can be combined. Notice that sum runs over all possible combinations with a constraints that powers of terms must sum to n. This result can also be shown by repeatedly using binomial theorem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --END--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunss.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsx.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunsi.otf');\n",
       "    }\n",
       "    @font-face {\n",
       "        font-family: \"Computer Modern\";\n",
       "        font-weight: bold;\n",
       "        font-style: oblique;\n",
       "        src: url('http://9dbb143991406a7c655e-aa5fcb0a5a4ec34cff238a2d56ca4144.r56.cf5.rackcdn.com/cmunso.otf');\n",
       "    }\n",
       "    div.cell{\n",
       "        width:800px;\n",
       "        margin-left:16% !important;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: Helvetica, serif;\n",
       "    }\n",
       "    h4{\n",
       "        margin-top:12px;\n",
       "        margin-bottom: 3px;\n",
       "       }\n",
       "    div.text_cell_render{\n",
       "        font-family: Computer Modern, \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;\n",
       "        line-height: 145%;\n",
       "        font-size: 130%;\n",
       "        width:800px;\n",
       "        margin-left:auto;\n",
       "        margin-right:auto;\n",
       "    }\n",
       "    .CodeMirror{\n",
       "            font-family: \"Source Code Pro\", source-code-pro,Consolas, monospace;\n",
       "    }\n",
       "    .prompt{\n",
       "        display: None;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 300;\n",
       "        font-size: 22pt;\n",
       "        color: #4057A1;\n",
       "        font-style: italic;\n",
       "        margin-bottom: .5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "    }\n",
       "\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "        }\n",
       "</style>\n",
       "\n",
       "<style>\n",
       "div.summary {\n",
       "    background-color: \t#F5F5F5;\n",
       "    border-color: #dFb5b4;\n",
       "    border-left: 0px solid #F08080;\n",
       "    border-right: 0px solid #F08080;\n",
       "    border-top: 0px solid #F08080;\n",
       "    border-bottom: 0px solid #F08080;\n",
       "    }\n",
       " </style>\n",
       "\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "def set_css_style(css_file_path):\n",
    "    styles = open(css_file_path, \"r\").read()\n",
    "    return HTML(styles)\n",
    "set_css_style('./common/custom.css')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
