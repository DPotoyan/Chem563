{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Maximum entropy distribution of die rolls with a constraint.\n",
    "\n",
    " We know that in the absence of constraints the probability distirbution of die rolls is uniform, each number shows up with 1/6 probability.  <br>\n",
    " (i) What would be the probability distribution of die rolls subject to a constraint that its first moment is fixed at some value $\\mu$. <br>\n",
    "(ii) What would be the probability distribution of die rolls subject to a constraint that its second moment is fixed to $\\sigma^2$. The procedure for deriving porbability distirbution by maximizing the entropy is exactly the same one that we used for deriving Boltzmann distribution. E.g you may take $E_i=i$ for an analogy and think of die values as energy levels.  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Porblem with constraints involve maximizing the entropy $-\\sum_i p_i log p_i$ combined with constraints via lagrange multipliers. \n",
    "\n",
    "(i) The constraint is on first momemnt, e.g if $x_i$ are die numbers then $\\sum_i p_i x_i=\\mu$. Hence we need to maximize $I(p,\\lambda)=-\\sum_i p_i log p_i-\\lambda (\\sum_i p_i x_i)$\n",
    "\n",
    "$$\\frac{\\partial I(p,\\lambda)}{\\partial p_i}=-1-log p_i - \\lambda x_i=0 $$\n",
    "\n",
    "$$p_i= e^{1-\\lambda x_i} $$\n",
    "\n",
    "Enforcing normalization via  $\\sum_i p_i = \\sum_i e^{1-\\lambda x_i}=1$ we get $$p_i=\\frac{e^{-\\lambda x_i}}{\\sum_i e^{-\\lambda x_i}}$$\n",
    "\n",
    "Finally since we have constraint $\\sum_i p_i x_i=\\mu$ we have the following familiar relationship between lagrange mulitplier $\\lambda$ and $\\mu$:\n",
    "\n",
    "$$ \\mu=\\frac{\\partial }{\\partial \\lambda}log \\sum_i e^{-\\lambda x_i} $$\n",
    "\n",
    "(ii) The constraint on the second moment $\\sum_i p_i x^2_i$ or the variance $\\sum_i p_i x^2_i-\\Big (\\sum_i p_i x_i\\Big )^2$ is realized just as easily. Our task is again reduced to maximizin the new functional:\n",
    "$$I(p,\\lambda)=-\\sum_i p_i log p_i-\\lambda (\\sum_i p_i x^2_i)$$\n",
    "\n",
    "$$\\frac{\\partial I(p,\\lambda)}{\\partial p_i}=-1-log p_i - \\lambda x^2_i=0 $$\n",
    "\n",
    "In complete analogy with the first part we get:\n",
    "\n",
    "$$p_i=\\frac{e^{-\\lambda x^2_i}}{\\sum_i e^{-\\lambda x^2_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Dervie the grand canonical ensemble from MaxEnt $\\mu V T$ \n",
    "(i) Maximizing entropy with constraints that average number of particle and average energy are fixed. <br>\n",
    "(ii) Derive expression for the particle number fluctuation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "This problem uses the same strategy as that of first problem of constraint entropy maximization:\n",
    "\n",
    "(i) We have two constraints, one for fixing mean energy $\\sum_{i} p(E_i,N_i) E_i=\\bar E$ and another for meant number of particles $\\sum_{i} p(E_i,N_i) N_i=\\bar N$ Where sumation over index $i$ is understood to colver all possible particle numbers and energy pairs $(N_i,E_i)$ <br>\n",
    "\n",
    "$$I(p,\\beta,\\lambda_1,\\lambda_2)=-\\sum_{i} p(E_i,N_i) log p(E_i,N_i)-\\lambda_1 \\sum_{i} p(E_i,N_i) E_i-\\lambda_2 \\sum_{i} p(E_i,N_i) N_i$$\n",
    "\n",
    "$$\\frac{\\partial I(p,\\beta,\\mu,\\lambda)}{\\partial p_i} =-1-log\\, p(E_i,N_i)- \\lambda_1 E_i- \\lambda_2 N_i=0 $$\n",
    "\n",
    "$$ p(E_i,N_i) \\sim e^{-\\lambda_1 E_i-\\lambda_2 N_i} $$\n",
    "\n",
    "After enforcing normalization and identifying lagrange multipliers as intensitve thermodynamic variables: $\\lambda_1=\\beta$ and $\\lambda_2=-\\beta \\mu$ we get:\n",
    "\n",
    "$$ p(E_i,N_i)=\\frac{e^{-\\beta (E_i-\\mu N_i)}}{\\Xi[\\mu,V,\\beta]} $$\n",
    "\n",
    "Where the normalizationis enforeced by grand canonical partition function: $\\Xi[\\mu,V,\\beta]=\\sum_{E_i,N_i}e^{-\\beta (E_i-\\mu N_i)}  $\n",
    "\n",
    "(ii) The log of Grand canonical partiion function is a cumulant generating function for energy and particle numbers. Hence to get fluctuation in particle number we need only differentiaite $log\\Xi$ twice with respect to its conjugate variable:\n",
    "\n",
    "$$ \\sigma^2_N=\\langle (N-\\bar N)^2 \\rangle = \\frac{\\partial^2}{\\partial (\\beta \\mu)^2} log\\Xi[\\mu,V,\\beta]=\\frac{\\partial \\bar N}{\\partial (\\beta \\mu)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Monty Hall and Entropy.\n",
    "\n",
    "We are playing the Monty hall again! Quantify the amount of information we gain in bits (difference in entropy) after we pick a random door and showman opens the remaining N doors from N+2 total: <br>\n",
    "(i) N=2 doors<br>\n",
    "(ii) N=99 doors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.0, 0.8112781244591328, 1.188721875540867)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "s1=-3/4*math.log(3/4,2)-1/4*math.log(1/4,2)\n",
    "math.log(4,2),s1,math.log(4,2)-s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.6582114827517955, 0.08013604733127526, 6.57807543542052)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1=-100/101*math.log(100/101,2)-1/101*math.log(1/101,2)\n",
    "math.log(101,2),s1,math.log(101,2)-s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "(i) Initially all the doors were equally likely to contain the prize hence: $S_i=log_2 4=2$ bits. \n",
    "\n",
    "After 2 doors were open we have the first door with probability of 1/4 containing prize and the second one with 1-1/4=3/4. Hence $S_f=-3/4log_2 3/4-1/4log_2 1/4=0.81$. \n",
    "\n",
    "Clearlyafter opening doors there is less uncertainty about where the prize is $S_f < S_i$ hence the amount of information that was obtained from showman is:  \n",
    "\n",
    "$$\\Delta S=S_i-S_f = 1.18 bits$$\n",
    "\n",
    "\n",
    "(ii) Similiar caluclation is done: $S_i = log_2 101=6.65$ and $S_f=-\\frac{100}{101}log_2 \\frac{100}{101}-\\frac{1}{101}log_2 \\frac{1}{101}=0.08$\n",
    "\n",
    "$$\\Delta S=S_i-S_f = 6.57 bits$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Informatin based view of thermodynamics.\n",
    "\n",
    "Acetic acid boils at T=391 and its latent heat of evaporation is $Q=23.7 kJ/mol$\n",
    "\n",
    "(i) What is the entropy increase due to evaporation. <br>\n",
    "(ii) How many Yes-no question shall we ask to pin down the single microscpic state of the system in the gas state as accruately as it is done in the lquid state? <br>\n",
    "(iii) For ethanol, entropy change during evaporation is $\\Delta S=110 J/K mol$ Compare the entropy difference with acetic acid and rationalize the difference in entropy changes keeping in mind that acetic acid makes stable dimers in gase phase while ethanol does not.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "(i) The entropy change due to evaporation is: $\\Delta S=\\frac{dQ}{T}=23700/391=60.6\\, J K^{-1}mol^{-1}$\n",
    "\n",
    "(ii) We need unit conversion, e.g $1 J K^{-1}mol^{-1} = 0.17$  bit/molecule. Hence the evaporation produced $60.6\\cdot 0.17=10.3$ bits. That is we need about 1 yes or no questions to determine the state of each molecule in gas with same precision as was known in liquid state. \n",
    "\n",
    "(iii) Ethanol evaporation corresponds to 19 bits/molecule  increae of entropy. Looks like the number of yes/no questions doubled in this case! Assuming that two tighly bound molecules behave as one in the gas phase of acetic acid then knowledge of one molecule is good enough for both molecules, so the obtained result is reasonable then (recall also that entropy in therodynamics is additive).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Informatin, DNA and horses.\n",
    "Compute the minimal number of bits we need to record the information in the short DNA sequence from a horse genome TGCACCCTCAGGT assuming: <br>\n",
    "(i) Nucletodie bases all have equal probabilities. <br>\n",
    "(ii) G and C hare twice more likely to be observed than A and T. <br> \n",
    "(iii) GG CC, AA or TT elements occur with twice lower probability than the other pairs. <br>\n",
    "(iv) We have 4 horses competing with different genomes and different probabilities to win. How would we encode the.  message of which horse won if each horse has mutation in DNA sequence that makes each faster by x,2x,3x,4x times. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.30296890880645"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq='TGCACCCTCAGGT '\n",
    "s=math.log(len(seq),2)\n",
    "S=len(seq)*s\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) $S=SequenceLength*log_2 (SequenceLength)=53.3$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 2, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.count('T'),seq.count('G'),seq.count('A'),seq.count('C')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(ii)  We have $p_c=p_g=2x$ and $p_a=p_t=x$ hence summing up probabilities we get x=1/6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.856141676762853"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=-2*1/3*math.log(1/3,2)-2*1/6*math.log(1/6,2) # Entropy per base\n",
    "S=len(seq)*s # Entropy of the entire sequence\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0, 0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq.count('GG'),seq.count('CC'),seq.count('TT'),seq.count('AA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq)/2 # number of pairs(call em Q and R) \n",
    "           #out of which 2 have twice less probability tha R, \n",
    "           #e.g 0.5x+0.5x+5x=1 so x=1/6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.107017709595356"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=-1/12*math.log(1/12,2)-1/6*math.log(1/6,2) # uncertainty/entropy about one pair\n",
    "7*s # number of yes/no to identify entire sequence of pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iv) $p_1=x, p_2=2x,p_3=3x,p_4=4x$ thus x=1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8464393446710154"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S=-1/10*math.log(1/10,2)-2/10*math.log(2/10,2)-3/10*math.log(3/10,2)-4/10*math.log(4/10,2)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
