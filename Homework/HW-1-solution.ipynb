{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution to problem 1.2 \n",
    "In lecture we found that binomial distribution of n; the number of sucesses(heads, moves to the right, etc) peaks around the average of n decaying rapidly with increasing N, total number of steps. This behaviour justifes (i) using continuous approximation for n and (ii) making taylor expansion around average of n : $\\langle n \\rangle$,\n",
    "\n",
    "$$B_N(n)=\\frac{N!}{n! (N-n)!}p^n (1-p)^{N-n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make continuous approximation for n and N by making use of Striling's expression for factorials: $logN!=NlogN-N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$logP_N(n)=NlogN -nlogn - (N-n)log(N-n)+nlog p +(N-n)log(1-p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute the first and second derivatives evaluated at the maximum of $n=\\langle n \\rangle$ to be used in Taylor expansion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dn}logP_N(n) \\Big |_{\\langle n \\rangle} =0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dn}logP_N(n)= -log n -1+1+log(N-n)+logp-log(1-p)=log \\frac{N- n }{ n}-log \\frac{1-p}{p}=0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$log \\frac{N-\\langle n\\rangle }{\\langle n\\rangle }=log \\frac{1-p}{p} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\langle n\\rangle = Np$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that average obtained in continuum approximation coincides with average of binomial distirbutions obtained via $\\langle n \\rangle =\\sum^{n=N}_{n=0} B_N(n) n=Np$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to taking second derivative:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{d^2}{dn^2}logP_N(n)= -\\frac{1}{(N-n)}-\\frac{1}{n}=-\\frac{N}{n(N-n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{d^2}{dn^2}logP_N(n)\\Big |_{\\langle n \\rangle}=-\\frac{1}{Np(1-p)}=-\\frac{1}{\\sigma_n^2} \\ll 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see that in continuum approximation the variance of n, $\\sigma_n^2$ coincides with variance of binomial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having both derivatives evaluated at the maximum of $n_{max}=\\langle n \\rangle$ we now proceed to making taylor expansion of the distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$logP_N(n) \\approx logP_N(\\langle n \\rangle)+\\frac{1}{2}\\frac{d^2}{dn^2}logP_N(n)\\Big |_{\\langle n \\rangle} (n-\\langle n \\rangle)^2+... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the first term is constant, first derivative is zero, second derivative gives quadratix approximation and all the higher derivatives are small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_N(n)= C exp \\Big[-\\frac{(n-\\langle n \\rangle)^2}{2\\sigma_n^2} \\Big] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant C is obtained by normalizing the distirbution $\\int dn P_N(n)=1 $ and it is $C=\\frac{1}{(2 \\pi \\sigma^2)^{1/2}}$. Thus we have found a continuous approximation to Binomian distribution, which is the all familiar normal distribution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P_N(n)=\\frac{1}{(2 \\pi \\sigma^2)^{1/2}} exp \\Big[-\\frac{(n-\\langle n \\rangle)^2}{2\\sigma_n^2} \\Big] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution to problem 1.5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture on random walk we derived this beuatiful result relating position of random walker (cumulative sum of all previous steps) to number of steps. The key quantity that emerge was the rate function I(x) $$ P(X_N=x)\\sim e^{-NI(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I(x)=\\frac{1+xN}{2}ln \\Big (\\frac{1+xN}{2}\\Big )+\\frac{1-xN}{2}ln\\Big ( \\frac{1-xN}{2}\\Big ) -\\frac{1+xN}{2}ln p-\\frac{1-xN}{2}ln q $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for p=q=\\frac{1}{2} case of symmetric random walk we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I(x)=\\frac{1+x/N}{2}ln \\Big (\\frac{1+x/N}{2}\\Big )+\\frac{1-x/N}{2}ln\\Big ( \\frac{1-x/N}{2}\\Big ) -\\frac{1+x/N}{2}ln p-\\frac{1-x/N}{2}ln q $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$I(x)=\\frac{1+x/N}{2}ln \\Big (\\frac{1+x/N}{2}\\Big )+\\frac{1-x/N}{2}ln\\Big ( \\frac{1-x/N}{2}\\Big )+log2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dx}I(x)=0=\\frac{1}{2N}+\\frac{1}{2N}ln \\Big (\\frac{1+x/N}{2}\\Big )  -\\frac{1}{2N} -\\frac{1}{2N}ln\\Big ( \\frac{1-x/N}{2}\\Big )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{min}=0=\\langle x \\rangle$$ an unsurprising result, but an important one nevertheless. The minima of I(x) is the exepectation of x, hence the power of large deviation theory: it is the deviations from the expectation value that are exponentially suppressed with the increasing number of trials. E.g an experiment of tossing coin 1000 times repeated over and over will yield a probability of deviation $\\delta$ from mean is: $e^{-1000\\delta}$. Tiny number, menaing that average has stabilized really really well around 1/2 value. This answers the question of part (ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now lets do taylor expansion and proice the answer for the part (i): $$I(x)=I(x_{min})+\\frac{1}{2}I''(x)(x-x_{min})^2=\\Big(\\frac{x}{N}\\Big)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above formula is just the right tool for computing the probability of deviation from average magnetiation $\\langle s \\rangle=0$: $$P(x)=Cexp(-Ns^2)$$ where s being the average magnetization $s=\\frac{1}{N}\\sum_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{P(\\langle s \\rangle+\\delta)}{P(\\langle s \\rangle)}= exp(-N\\delta^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wee see that while for $N=10^3$ the deviation of $\\delta=10^{-6}$ can be appreicable $exp(-10^{-9}) \\approx 1$ it is virutally unbservable for macroscopic system $exp(-10^{11}) \\approx 0$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
